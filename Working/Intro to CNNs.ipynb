{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/.hub_local/python_libs/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /home/.hub_local/python_libs/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/.hub_local/python_libs/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/.hub_local/python_libs/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/.hub_local/python_libs/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",

      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",


   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes extensive use of examples and figures from [here](http://cs231n.github.io/convolutional-networks/), which is a great reference for further details."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "train_ds = tfds.load('fashion_mnist', split='train')\n",
    "test_ds = tfds.load('fashion_mnist', split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST - Fashion\n",
    "\n",
    "... a collection of small 28x28 pixel images of various pieces of clothing. This is a multi-class classification problem, identify the type of object in the image\n",
    "\n",
    "|Label| Class  |\n",
    "|------ | ------|\n",
    "|    0|T-shirt/top|\n",
    "|    1|Trouser|\n",
    "|    2| Pullover|\n",
    "|    3| Dress|\n",
    "|    4| Coat|\n",
    "|    5| Sandal|\n",
    "|    6| Shirt|\n",
    "|    7| Sneaker|\n",
    "|    8| Bag|\n",
    "|    9| Ankle boot|"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "# This is useful for making plots it takes an integer\n",
    "lookup_dict={\n",
    "    0:'T-shirt/top',\n",
    "    1 :'Trouser',\n",
    "    2 :'Pullover',\n",
    "    3 :'Dress',\n",
    "    4 :'Coat',\n",
    "    5 :'Sandal',\n",
    "    6 :'Shirt',\n",
    "    7 :'Sneaker',\n",
    "    8 :'Bag',\n",
    "    9 :'Ankle boot'\n",
    "}\n",
    "\n",
    "labels = list(lookup_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularize the data."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda sample: {'image': tf.cast(sample['image'],\n",
    "                                                        tf.float32) / 255.,\n",
    "                                        'label': sample['label']}) # normalize train set\n",
    "test_ds = test_ds.map(lambda sample: {'image': tf.cast(sample['image'],\n",
    "                                                        tf.float32) / 255.,\n",
    "                                    'label': sample['label']}) # normalize test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch up the data."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0) # ensure dataset shuffling"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# shuffle data by allocating a buffer of 1024 to randomly draw examples from\n",
    "train_ds = train_ds.repeat(num_epochs).shuffle(1024)\n",
    "test_ds = test_ds.shuffle(1024)\n",
    "\n",
    "# group into batches of 32, skip incomplete batch, prefetch next sample to speed up\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(nn.Module):\n",
    "  \"\"\"A simple model with densely connected layers.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = x.reshape((x.shape[0], -1))   # flatten\n",
    "    x = nn.Dense(features=100)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=100)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=n_classes)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                DenseNN Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams            \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ DenseNN │ \u001b[2mfloat32\u001b[0m[1,28,28,1] │ \u001b[2mfloat32\u001b[0m[1,10]  │                    │\n",
      "├─────────┼─────────┼────────────────────┼────────────────┼────────────────────┤\n",
      "│ Dense_0 │ Dense   │ \u001b[2mfloat32\u001b[0m[1,784]     │ \u001b[2mfloat32\u001b[0m[1,100] │ bias: \u001b[2mfloat32\u001b[0m[100] │\n",
      "│         │         │                    │                │ kernel:            │\n",
      "│         │         │                    │                │ \u001b[2mfloat32\u001b[0m[784,100]   │\n",
      "│         │         │                    │                │                    │\n",
      "│         │         │                    │                │ \u001b[1m78,500 \u001b[0m\u001b[1;2m(314.0 KB)\u001b[0m  │\n",
      "├─────────┼─────────┼────────────────────┼────────────────┼────────────────────┤\n",
      "│ Dense_1 │ Dense   │ \u001b[2mfloat32\u001b[0m[1,100]     │ \u001b[2mfloat32\u001b[0m[1,100] │ bias: \u001b[2mfloat32\u001b[0m[100] │\n",
      "│         │         │                    │                │ kernel:            │\n",
      "│         │         │                    │                │ \u001b[2mfloat32\u001b[0m[100,100]   │\n",
      "│         │         │                    │                │                    │\n",
      "│         │         │                    │                │ \u001b[1m10,100 \u001b[0m\u001b[1;2m(40.4 KB)\u001b[0m   │\n",
      "├─────────┼─────────┼────────────────────┼────────────────┼────────────────────┤\n",
      "│ Dense_2 │ Dense   │ \u001b[2mfloat32\u001b[0m[1,100]     │ \u001b[2mfloat32\u001b[0m[1,10]  │ bias: \u001b[2mfloat32\u001b[0m[10]  │\n",
      "│         │         │                    │                │ kernel:            │\n",
      "│         │         │                    │                │ \u001b[2mfloat32\u001b[0m[100,10]    │\n",
      "│         │         │                    │                │                    │\n",
      "│         │         │                    │                │ \u001b[1m1,010 \u001b[0m\u001b[1;2m(4.0 KB)\u001b[0m     │\n",
      "├─────────┼─────────┼────────────────────┼────────────────┼────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m89,610 \u001b[0m\u001b[1;2m(358.4 KB)\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴─────────┴────────────────────┴────────────────┴────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 89,610 \u001b[0m\u001b[1;2m(358.4 KB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "\n",
    "dummy_input = jnp.ones((1, 28, 28, 1))\n",
    "\n",
    "dnn = DenseNN()\n",
    "print(dnn.tabulate(jax.random.PRNGKey(0), jnp.ones((1, 28, 28, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare for training."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import metrics\n",
    "from flax.training import train_state\n",
    "from flax import struct\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "   metrics: Metrics\n",
    "\n",
    "# state = create_train_state(dnn, init_rng, learning_rate, momentum)\n",
    "def create_train_state(model, rng, learning_rate):\n",
    "    params = model.init(rng, dummy_input)['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx,\n",
    "        metrics=Metrics.empty())\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn({'params': params}, batch['image'])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "    return loss\n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    logits = state.apply_fn({'params': state.params}, batch['image'])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=logits, labels=batch['label'], loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "init_rng = jax.random.PRNGKey(0)\n",
    "learning_rate = 0.01\n",
    "state = create_train_state(dnn, init_rng, learning_rate)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since train_ds is replicated num_epochs times in get_datasets(), we divide by num_epochs\n",
    "num_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs\n",
    "num_steps_per_epoch"
   ]
  },
  {
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 0.47712472081184387, accuracy: 82.55332946777344\n",
      "test epoch: 1, loss: 0.4476986527442932, accuracy: 84.06449890136719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 2, loss: 0.4003068804740906, accuracy: 85.413330078125\n",
      "test epoch: 2, loss: 0.44380608201026917, accuracy: 84.13461303710938\n",
      "train epoch: 3, loss: 0.37538737058639526, accuracy: 86.41832733154297\n",
      "test epoch: 3, loss: 0.42769700288772583, accuracy: 84.74559020996094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 4, loss: 0.366230845451355, accuracy: 86.87833404541016\n",
      "test epoch: 4, loss: 0.4430948793888092, accuracy: 84.25480651855469\n",
      "train epoch: 5, loss: 0.36366474628448486, accuracy: 86.94666290283203\n",
      "test epoch: 5, loss: 0.4509197175502777, accuracy: 85.23637390136719\n",
      "train epoch: 6, loss: 0.3627506494522095, accuracy: 87.01666259765625\n",
      "test epoch: 6, loss: 0.4348699748516083, accuracy: 85.05609130859375\n",
      "train epoch: 7, loss: 0.3522688150405884, accuracy: 87.2249984741211\n",
      "test epoch: 7, loss: 0.4471062421798706, accuracy: 85.42668151855469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 8, loss: 0.34970298409461975, accuracy: 87.47833251953125\n",
      "test epoch: 8, loss: 0.4750124216079712, accuracy: 84.16465759277344\n",
      "train epoch: 9, loss: 0.3541569411754608, accuracy: 87.37999725341797\n",
      "test epoch: 9, loss: 0.4292280375957489, accuracy: 85.07612609863281\n",
      "train epoch: 10, loss: 0.3418242335319519, accuracy: 87.84500122070312\n",
      "test epoch: 10, loss: 0.4366139769554138, accuracy: 85.88742065429688\n",
      "train epoch: 11, loss: 0.33816877007484436, accuracy: 87.83833312988281\n",
      "test epoch: 11, loss: 0.44570180773735046, accuracy: 85.927490234375\n",
      "train epoch: 12, loss: 0.33501487970352173, accuracy: 88.04000091552734\n",
      "test epoch: 12, loss: 0.4654588997364044, accuracy: 84.5352554321289\n",
      "train epoch: 13, loss: 0.33295395970344543, accuracy: 87.9433364868164\n",
      "test epoch: 13, loss: 0.45758673548698425, accuracy: 84.97596740722656\n",
      "train epoch: 14, loss: 0.32996147871017456, accuracy: 88.20833587646484\n",
      "test epoch: 14, loss: 0.49284684658050537, accuracy: 84.74559020996094\n",
      "train epoch: 15, loss: 0.3333531320095062, accuracy: 88.05333709716797\n",
      "test epoch: 15, loss: 0.46368446946144104, accuracy: 85.47676086425781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 16, loss: 0.32941824197769165, accuracy: 88.19000244140625\n",
      "test epoch: 16, loss: 0.47789672017097473, accuracy: 85.08613586425781\n",
      "train epoch: 17, loss: 0.3202965259552002, accuracy: 88.45833587646484\n",
      "test epoch: 17, loss: 0.4703806936740875, accuracy: 84.43509674072266\n",
      "train epoch: 18, loss: 0.32001131772994995, accuracy: 88.51333618164062\n",
      "test epoch: 18, loss: 0.45192426443099976, accuracy: 86.21794891357422\n",
      "train epoch: 19, loss: 0.3180304765701294, accuracy: 88.57499694824219\n",
      "test epoch: 19, loss: 0.4801791310310364, accuracy: 85.05609130859375\n",
      "train epoch: 20, loss: 0.32560598850250244, accuracy: 88.40833282470703\n",
      "test epoch: 20, loss: 0.5103766322135925, accuracy: 84.89582824707031\n",
      "train epoch: 21, loss: 0.3240489363670349, accuracy: 88.45166778564453\n",
      "test epoch: 21, loss: 0.47245338559150696, accuracy: 84.87580108642578\n",
      "train epoch: 22, loss: 0.3194273114204407, accuracy: 88.57833099365234\n",
      "test epoch: 22, loss: 0.4867140054702759, accuracy: 85.927490234375\n",
      "train epoch: 23, loss: 0.3094500005245209, accuracy: 88.66166687011719\n",
      "test epoch: 23, loss: 0.46549859642982483, accuracy: 85.556884765625\n",
      "train epoch: 24, loss: 0.3161938190460205, accuracy: 88.66000366210938\n",
      "test epoch: 24, loss: 0.4691709876060486, accuracy: 86.05769348144531\n",
      "train epoch: 25, loss: 0.31491339206695557, accuracy: 88.62166595458984\n",
      "test epoch: 25, loss: 0.5323187708854675, accuracy: 85.66706848144531\n",
      "train epoch: 26, loss: 0.3125610053539276, accuracy: 88.7316665649414\n",
      "test epoch: 26, loss: 0.47172456979751587, accuracy: 85.86738586425781\n",
      "train epoch: 27, loss: 0.30579444766044617, accuracy: 88.97833251953125\n",
      "test epoch: 27, loss: 0.5764041543006897, accuracy: 84.01441955566406\n",
      "train epoch: 28, loss: 0.3113691210746765, accuracy: 88.80000305175781\n",
      "test epoch: 28, loss: 0.546544075012207, accuracy: 85.83734130859375\n",
      "train epoch: 29, loss: 0.3061448931694031, accuracy: 88.8800048828125\n",
      "test epoch: 29, loss: 0.4760977029800415, accuracy: 86.0977554321289\n",
      "train epoch: 30, loss: 0.3105769753456116, accuracy: 88.80166625976562\n",
      "test epoch: 30, loss: 0.557714581489563, accuracy: 84.12460327148438\n",
      "train epoch: 31, loss: 0.30742329359054565, accuracy: 88.9366683959961\n",
      "test epoch: 31, loss: 0.5054083466529846, accuracy: 86.07772827148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 32, loss: 0.3079306185245514, accuracy: 88.91500091552734\n",
      "test epoch: 32, loss: 0.5146930813789368, accuracy: 85.9375\n",
      "train epoch: 33, loss: 0.2997405230998993, accuracy: 89.26000213623047\n",
      "test epoch: 33, loss: 0.5511378645896912, accuracy: 85.66706848144531\n",
      "train epoch: 34, loss: 0.2991810142993927, accuracy: 89.2066650390625\n",
      "test epoch: 34, loss: 0.59571772813797, accuracy: 86.04767608642578\n",
      "train epoch: 35, loss: 0.30643901228904724, accuracy: 88.94332885742188\n",
      "test epoch: 35, loss: 0.5131513476371765, accuracy: 86.03765869140625\n",
      "train epoch: 36, loss: 0.303219199180603, accuracy: 89.11833190917969\n",
      "test epoch: 36, loss: 0.5155875086784363, accuracy: 86.1378173828125\n",
      "train epoch: 37, loss: 0.31456685066223145, accuracy: 88.71333312988281\n",
      "test epoch: 37, loss: 0.5438756942749023, accuracy: 85.66706848144531\n",
      "train epoch: 38, loss: 0.300052285194397, accuracy: 89.09166717529297\n",
      "test epoch: 38, loss: 0.5106889009475708, accuracy: 85.18629455566406\n",
      "train epoch: 39, loss: 0.3039212226867676, accuracy: 89.125\n",
      "test epoch: 39, loss: 0.5683515071868896, accuracy: 84.74559020996094\n",
      "train epoch: 40, loss: 0.3012259006500244, accuracy: 89.14833068847656\n",
      "test epoch: 40, loss: 0.6214732527732849, accuracy: 85.7071304321289\n",
      "train epoch: 41, loss: 0.29644396901130676, accuracy: 89.26666259765625\n",
      "test epoch: 41, loss: 0.5631499886512756, accuracy: 86.26802825927734\n",
      "train epoch: 42, loss: 0.2969755232334137, accuracy: 89.25833129882812\n",
      "test epoch: 42, loss: 0.5588818192481995, accuracy: 86.03765869140625\n",
      "train epoch: 43, loss: 0.2959890365600586, accuracy: 89.29666900634766\n",
      "test epoch: 43, loss: 0.5603984594345093, accuracy: 84.16465759277344\n",
      "train epoch: 44, loss: 0.29974380135536194, accuracy: 89.23332977294922\n",
      "test epoch: 44, loss: 0.6202290654182434, accuracy: 85.30648803710938\n",
      "train epoch: 45, loss: 0.29812881350517273, accuracy: 89.2300033569336\n",
      "test epoch: 45, loss: 0.5880427360534668, accuracy: 84.88581848144531\n",
      "train epoch: 46, loss: 0.29286572337150574, accuracy: 89.25\n",
      "test epoch: 46, loss: 0.6155200004577637, accuracy: 85.60697174072266\n",
      "train epoch: 47, loss: 0.2946171760559082, accuracy: 89.45000457763672\n",
      "test epoch: 47, loss: 0.6817640066146851, accuracy: 83.86418151855469\n",
      "train epoch: 48, loss: 0.2991959750652313, accuracy: 89.32333374023438\n",
      "test epoch: 48, loss: 0.5939440727233887, accuracy: 85.80729675292969\n",
      "train epoch: 49, loss: 0.2924165427684784, accuracy: 89.35833740234375\n",
      "test epoch: 49, loss: 0.627604603767395, accuracy: 85.99759674072266\n",
      "train epoch: 50, loss: 0.30281874537467957, accuracy: 89.22332763671875\n",
      "test epoch: 50, loss: 0.6757448315620422, accuracy: 85.75721740722656\n"
     ]
    }
   ],
   "source": [
    "for step,batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "\n",
    "  # Run optimization steps over training batches and compute batch metrics\n",
    "  state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n",
    "  state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n",
    "\n",
    "  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n",
    "    for metric,value in state.metrics.compute().items(): # compute metrics\n",
    "      metrics_history[f'train_{metric}'].append(value) # record metrics\n",
    "    state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "    # Compute metrics on the test set after each training epoch\n",
    "    test_state = state\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      test_state = compute_metrics(state=test_state, batch=test_batch)\n",
    "\n",
    "    for metric,value in test_state.metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n",
    "    print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a fair amount of training we found a ~$86\\%$ validation accuracy.  OK, but not great.  Let's take another look at the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first layer has $28 \\times 28 \\times 1 \\times 100 = 78,500$ weights, which is manageable.  What if we had higher-resolution images?  Let's go with a modest $300 \\times 300$ pixel input.  That would be $300 \\times 300 \\times 3 \\times 100= 27$ Million parameters for one layer. This is still possible with modern GPUs, but does not generally yield good results. We need a solution that scales better with the number of pixels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $300\\times300\\times3$ pixel image:\n",
    "<img src=\"https://raw.githubusercontent.com/jsearcy1/racsml/master/assets/small_img.jpg\" style=\"width:100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully connected neural network becomes cumbersome when we start considering image recognition; it needs *lots* of weights, and isn't designed to take advantage of spatial features in the inputs.  This is where CNNs become powerful.\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\" style=\"width:100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs preserve the spatial (2-D) information of the input images, add a depth to their layers, and reduce the number of connections (and weights).\n",
    "\n",
    "The layers used to build CNN *architectures* fall into three categories:\n",
    " 1. Convolutional Layer\n",
    " 1. Pooling Layer\n",
    " 1. Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/cnn/depthcol.jpeg\" style=\"width=100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers consist of a set of filters that apply over a small spatial area, but the full depth of the input.  The example above shows on the left a [32x32x3] input volume (width 32, height 32 image with RGB color channels).  The volume on the right is an example of a convolutional layer, with a particular **depth column** highlighted which takes as input *only* the highlighted region of the input volume.  The spatial extent of the area covered by a depth column is referred to as the **receptive field**.\n",
    "\n",
    "The dimensions of the *output volume* are decided by 3 hyperparameters: **depth**, **stride**, and **zero-padding**.  We can compute the size of the output volume based on the volume size $W$, the receptive field size of the convolution layer $F$, their applied stride $S$, and the amount of zero padding used $P$.\n",
    "\n",
    "Based on these parameters, the number of pixels in the output of a convolutional layer is $(W−F+2P)/S+1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/cnn/stride.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a 1-D example with inputs on the bottom in blue ($W=5$) with a padding of 1 ($P=1$), and two different examples of a convolutional filter, both with receptive field $F=3$.\n",
    "\n",
    "*Left*: This is a convolutional layer with stride $S = 1$, meaning we expect ($5 - 3 + 2)/1+1 = 5$ neurons (i.e., outputs).\n",
    "\n",
    "*Right*: Layer with stride $S = 2$, meaning an output of size $(5 - 3 + 2)/2+1 = 3$.\n",
    "\n",
    "Note how on the left the zero padding allowed us to have the same number of outputs as inputs.  This is a common use of zero-padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dramatically reduce the number of parameters involved in a convolutional layer by making the assumption that if one feature is useful to compute at some spatial position $(x,y)$, then it should also be useful to compute at a different position $(x_2,y_2)$.\n",
    "\n",
    "In other words, denoting a single 2-dimensional slice of depth as a depth slice (e.g. a volume of size [7x7x3] has 3 depth slices, each of size [7x7]), we are going to constrain the pixels in each depth slice to use the same weights and bias. All $7\\times7$ pixels in each depth slice will now be using the same parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://harishnarayanan.org/images/writing/artistic-style-transfer/conv-layer.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets look at the how the first output pixel is calculated\n",
    "\n",
    "|X[0:3,0:3,0]|W0[:,:,0]| \n",
    "|------ | ------|\n",
    "|0,0,0  |-1,0,1|\n",
    "|0,0,0  |0,0,1 |\n",
    "|0,1,0  |1,-1,1|\n",
    "|first channel = | -1|\n",
    "+\n",
    "|X[0:3,0:3,1]|W0[:,:,1]|\n",
    "|0,0,0  |-1,0,1|\n",
    "|0,2,1  |1,-1,1|\n",
    "|0,2,1  |0,1,0|\n",
    "|second channel = |1|\n",
    "+\n",
    "|X[0:3,0:3,2]|W0[:,:,2]|\n",
    "|0,0,0  |-1,1,1|\n",
    "|0,2,1  |1,1,0 |\n",
    "|0,1,0  |0,-1,0|\n",
    "|third channel= | 1|\n",
    "|bias = |1 |\n",
    "|Sum Total| 2 = O[0,0,0]|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers act to reduce the dimension of the propagated volume, reducing the number of weights going forward, reducing cost and reducing the chances of over-fitting.  It operates independently on each depth slice.  A common operation is to take the MAX over a region.\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/pool.jpeg\" style=\"width:300\">\n",
    "\n",
    "For example,\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" style=\"width:500\">\n",
    "\n",
    "**NOTE**: These are being used less and less these days, in favor of other methods such as strided convolutions seen above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the MNIST fasion data set.  First we'll download some external images that will be useful for visualizations later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shape\n",
    "\n",
    "Convolutional layers expect to handle 3-D data cubes, so we'll need to do a little reshaping of our dataset."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': TensorSpec(shape=(32, 28, 28, 1), dtype=tf.float32, name=None),\n",
       " 'label': TensorSpec(shape=(32,), dtype=tf.int64, name=None)}"
      ]
     },

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now build our (sequential) model using `Flax`."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "class ConvNN(nn.Module):\n",
    "  \"\"\"A simple model with densely connected layers.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, training: bool):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Dropout(rate=0.25, deterministic=not training)(x)\n",
    "    x = x.reshape((x.shape[0], -1))   # flatten\n",
    "    x = nn.Dense(features=128)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(rate=0.5, deterministic=not training)(x)\n",
    "    x = nn.Dense(features=n_classes)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first layer (after the input layer) will be a convolutional layer with `32` filters.\n",
    "* Conv( Filters, (filter height, filter width), strides=1, padding='None' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add on another `32`-filter convolutional layer, followed by a 2-D pooling layer with a pooling size of `2x2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add on a *dropout* layer.  This layer will set the specified fraction of inputs to `0`, which can be useful for avoiding over fitting.  We'll drop $25\\%$ of the inputs with this layer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add a fully connected layer with `128` nodes, making sure to flatten the volume before passing to this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now add one final $25\\%$ dropout layer for good measure, followed by the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add our output layer.  Like with the dense netwok above, we need one node be possible category, and we'll use the `softmax` activation function since we're building a multi-class classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the final network."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                 ConvNN Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams          \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│           │ ConvNN  │ -                │ \u001b[2mfloat32\u001b[0m[1,10]    │                  │\n",
      "│           │         │ \u001b[2mfloat32\u001b[0m[1,28,28… │                  │                  │\n",
      "│           │         │ - training:      │                  │                  │\n",
      "│           │         │ False            │                  │                  │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│ Conv_0    │ Conv    │ \u001b[2mfloat32\u001b[0m[1,28,28… │ \u001b[2mfloat32\u001b[0m[1,28,28… │ bias:            │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[32]      │\n",
      "│           │         │                  │                  │ kernel:          │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[3,3,1,3… │\n",
      "│           │         │                  │                  │                  │\n",
      "│           │         │                  │                  │ \u001b[1m320 \u001b[0m\u001b[1;2m(1.3 KB)\u001b[0m     │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│ Conv_1    │ Conv    │ \u001b[2mfloat32\u001b[0m[1,28,28… │ \u001b[2mfloat32\u001b[0m[1,28,28… │ bias:            │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[32]      │\n",
      "│           │         │                  │                  │ kernel:          │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[3,3,32,… │\n",
      "│           │         │                  │                  │                  │\n",
      "│           │         │                  │                  │ \u001b[1m9,248 \u001b[0m\u001b[1;2m(37.0 KB)\u001b[0m  │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│ Dropout_0 │ Dropout │ \u001b[2mfloat32\u001b[0m[1,14,14… │ \u001b[2mfloat32\u001b[0m[1,14,14… │                  │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│ Dense_0   │ Dense   │ \u001b[2mfloat32\u001b[0m[1,6272]  │ \u001b[2mfloat32\u001b[0m[1,128]   │ bias:            │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[128]     │\n",
      "│           │         │                  │                  │ kernel:          │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[6272,12… │\n",
      "│           │         │                  │                  │                  │\n",
      "│           │         │                  │                  │ \u001b[1m802,944 \u001b[0m\u001b[1;2m(3.2 MB)\u001b[0m │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│ Dropout_1 │ Dropout │ \u001b[2mfloat32\u001b[0m[1,128]   │ \u001b[2mfloat32\u001b[0m[1,128]   │                  │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│ Dense_1   │ Dense   │ \u001b[2mfloat32\u001b[0m[1,128]   │ \u001b[2mfloat32\u001b[0m[1,10]    │ bias:            │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[10]      │\n",
      "│           │         │                  │                  │ kernel:          │\n",
      "│           │         │                  │                  │ \u001b[2mfloat32\u001b[0m[128,10]  │\n",
      "│           │         │                  │                  │                  │\n",
      "│           │         │                  │                  │ \u001b[1m1,290 \u001b[0m\u001b[1;2m(5.2 KB)\u001b[0m   │\n",
      "├───────────┼─────────┼──────────────────┼──────────────────┼──────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m813,802 \u001b[0m\u001b[1;2m(3.3 MB)\u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────┴─────────┴──────────────────┴──────────────────┴──────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 813,802 \u001b[0m\u001b[1;2m(3.3 MB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "\n",
    "cnn = ConvNN()\n",
    "print(cnn.tabulate(jax.random.PRNGKey(0), dummy_input, training=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that we have $813802$ trainable parameters in this network, compared to the $89610$ parameters in the dense network we constructed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've built our full model it's time to compile it, specifying the loss function, optimizer, and any metrics we want to look at, and fit on the training data."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "init_rng, dropout_rng = jax.random.split(jax.random.PRNGKey(0), 2)"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "   key: jax.Array\n",
    "   metrics: Metrics\n",
    "\n",
    "def create_train_state(model, rng, learning_rate):\n",
    "    params = model.init(rng, dummy_input, training=False)['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply, params=params,\n",
    "        key=dropout_rng, tx=tx,\n",
    "        metrics=Metrics.empty())\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, dropout_rng):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  dropout_train_key = jax.random.fold_in(key=dropout_rng, data=state.step)\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn({'params': params}, batch['image'],\n",
    "                            training=True, rngs={'dropout': dropout_train_key})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "    return loss\n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    logits = state.apply_fn({'params': state.params}, batch['image'], training=False)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(\n",
    "        logits=logits, labels=batch['label'], loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "# momentum = 0.9\n",
    "\n",
    "state = create_train_state(cnn, init_rng, learning_rate)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': []}"
   ]
  },
  {
   "cell_type": "code",

   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "# since train_ds is replicated num_epochs times in get_datasets(), we divide by num_epochs\n",
    "num_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs\n",
    "num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 0.4155186712741852, accuracy: 84.92666625976562\n",

     ]
    }
   ],
   "source": [
    "for step,batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "\n",
    "  # Run optimization steps over training batches and compute batch metrics\n",
    "  state = train_step(state, batch, dropout_rng) # get updated train state (which contains the updated parameters)\n",
    "  state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n",
    "\n",
    "  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n",
    "    for metric,value in state.metrics.compute().items(): # compute metrics\n",
    "      metrics_history[f'train_{metric}'].append(value) # record metrics\n",
    "    state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "    # Compute metrics on the test set after each training epoch\n",
    "    test_state = state\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      test_state = compute_metrics(state=test_state, batch=test_batch)\n",
    "\n",
    "    for metric,value in test_state.metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n",
    "    print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy in subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.set_title('Loss')\n",
    "ax2.set_title('Accuracy')\n",
    "for dataset in ('train','test'):\n",
    "  ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "  ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "ax1.legend()\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(state, batch):\n",
    "  logits = state.apply_fn({'params': state.params}, batch['image'], training=False)\n",
    "  return logits.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Lets look at how the model makes predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_ds.as_numpy_iterator():\n",
    "    pred = predict(state, batch)\n",
    "    logits = state.apply_fn({'params': state.params}, batch['image'], training=False)\n",
    "    test_idx = np.random.randint(len(pred))\n",
    "    print(\"outputs:\")\n",
    "    for i, p in enumerate(logits[test_idx]):\n",
    "        print(lookup_dict[i], round(p,2))\n",
    "\n",
    "    print('best guess:', pred)\n",
    "    plt.imshow(np.squeeze(batch['image'][test_idx]),cmap='gray')\n",
    "\n",
    "    print('Truth Class:',batch['label'][test_idx],lookup_dict[batch['label'][test_idx]]);\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Other Checks\n",
    "\n",
    "### Confusion matrix\n",
    "A confusion matrix is a 2-D histogram with the dimensions being the true class, and the predicted class.\n",
    "The diagonal bins in this histogram are correct prediction true_class==predicted_class, otherwise it is an\n",
    "incorrect prediction. Run the cell below and see if you can guess which class is hardest to identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(state, batch):\n",
    "    logits = state.apply_fn({'params': state.params}, batch['image'], training=False)\n",
    "    pred = predict(state, batch)\n",
    "    confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "\n",
    "    for truth, guess in zip(batch['label'], pred):\n",
    "        confusion_matrix[truth, guess] += 1\n",
    "\n",
    "    plt.imshow(confusion_matrix)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xticks(range(0,10), labels, rotation=90)\n",
    "    plt.yticks(range(0,10), labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_ds.as_numpy_iterator():\n",
    "    check(state, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
