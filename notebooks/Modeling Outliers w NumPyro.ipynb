{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import infer\n",
    "numpyro.set_host_device_count(2) # let's use 2 cores!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression w/ Outliers\n",
    "\n",
    "This notebook ended up mirroring much of the work in [Dan Foreman-Mackey's blog post](https://dfm.io/posts/intro-to-numpyro/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again use some synthetic data $(x_i,y_i)$, where $\\{x_i\\}$ are known with negligible uncertainty, and $\\{y_i\\}$ values have variable uncertainties $\\{\\sigma_{yi}\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with outliers we need to define a *generative model* for both the good data (foreground) and the bad (background).  Each datum can either be from the foreground or the background, and since we don't know *a priori*, we need to introduce discrete parameters $q_i$, where $q_i=0$ indicates datum $i$ is background, and $q_i=1$ if foreground.  We'll need to infer the probablility $P_b$ that any particular datum is part of the background.  Since we need a generative model for the background, let's assume it's normally distributed with mean and standard deviation $\\mu_b$ and $\\sigma_b$, independent of $x$.\n",
    "\n",
    "We have now added $N+3$ extra parameters $(\\{q_i\\}_{i=1}^N, P_b, \\mu_b, \\sigma_b)$ to deal with outliers, and the likelihood takes the form\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = p\\left(\\{y_i\\}_{i=1}^N|m,b,\\{q_i\\}_{i=1}^N,\\mu_b,\\sigma_b,I\\right)\n",
    "$$\n",
    "\n",
    "or, broken down into foreground and background\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_i \\left[p_\\mathrm{fg}(\\{y_i\\}|m,b,I)\\right]^{q_i} \\left[p_\\mathrm{bg}(\\{y_i\\}|\\mu_b, \\sigma_b, I)\\right]^{1-q_i},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "p_\\mathrm{fg} = \\frac{1}{\\sqrt{2\\pi\\sigma_{yi}^2}} \\exp \\left(-\\frac{(y_i - mx_i - b)^2}{2\\sigma_{yi}^2}\\right) \\\\\n",
    "p_\\mathrm{bg} = \\frac{1}{\\sqrt{2\\pi(\\sigma_b^2 + \\sigma_{yi}^2})} \\exp \\left(-\\frac{(y_i - \\mu_b)^2}{2(\\sigma_b^2 +\\sigma_{yi}^2)}\\right)\n",
    "$$\n",
    "\n",
    "Since we are specifying a probability $P_b$ of each data point being an outlier, we need to incorporate this binomial probability in our prior\n",
    "\n",
    "$$\n",
    "p(m,b,\\{q_i\\},P_b,\\mu_b,\\sigma_b|I) = p(\\{q_i\\}|P_b,I)p(m,b,P_b,\\mu_b,\\sigma_b|I)\n",
    "$$\n",
    "\n",
    "where $p(\\{q_i\\}|P_b,I)$ is the binomial probability\n",
    "\n",
    "$$\n",
    "p(\\{q_i\\}|P_b,I) = \\prod_i \\left(1 - P_b\\right)^{q_i}P_b^{1-q_i}\n",
    "$$\n",
    "\n",
    "We could sample in the (very) high-dimensional space, sampling both the continuous variables and the discrete ones.  However, we can also _marginalize_ over the $\\{q_i\\}$'s analytically, and save our MCMC some work.\n",
    "\n",
    "Focusing on our likelihood\n",
    "\n",
    "$$\n",
    "p\\left(\\{y_i\\}_{i=1}^N|m,b,\\{q_i\\}_{i=1}^N,\\mu_b,\\sigma_b\\right) = \\prod_i p(y_i|m,b,q_i,\\mu_b,\\sigma_b),\n",
    "$$\n",
    "\n",
    "we can integrate over (actually sum, since it's discrete) the unknown $q_i$'s to account for their uncertainty.  To do so we need to specify our prior $p(q_i)$ and we are left with:\n",
    "\n",
    "$$\n",
    "p\\left(\\{y_i\\}_{i=1}^N|m,b,\\{q_i\\}_{i=1}^N,\\mu_b,\\sigma_b\\right) = \\prod_i \\left [p(q_i=0) p(y_i|m,b,\\mu_b,\\sigma_b, q_i=0) + p(q_i=1) p(y_i|m,b,\\mu_b,\\sigma_b, q_i=1)\\right],\n",
    "$$\n",
    "\n",
    "which is a _mixture model_!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreground-only Model\n",
    "\n",
    "First let's repeat our previous analysis that assumed all data to be from the foreground model, i.e., following the linear relationship.  Before we let numpyro's Normal distribution implicitly vectorize the likelihood, which assumes each data point to be independent.  In general, the way to tell numpyro that values along a particular dimensions should be treated independently is to use a _plate_.  We'll use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x=None, y=None, σ_y=None):\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the lines corresponding to a few posterior draws to get a sense of the uncertainty in the line we're fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before improving our model, let's exercise one of numpyro superpowers, which is _predictive_ sampling, either by sampling our prior, or _posterior predictive sampling_, where we \"predict\" the observations based on our posterior estimates of the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it painfully obvious that our model is _not_ a good descriptor of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreground and Background Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_w_outliers(x=None, y=None, σ_y=None):\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution probabilities of each point being in the foreground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
